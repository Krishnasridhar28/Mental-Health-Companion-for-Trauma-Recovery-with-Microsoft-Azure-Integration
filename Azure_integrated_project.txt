# Final Year Project: Mental Health Companion using Microsoft Azure and AI
# Multi-modal Emotion Recognition System (Speech, Facial, and Posture Analysis)

import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from keras.models import load_model
import librosa
import sounddevice as sd
import azure.cognitiveservices.speech as speechsdk

# Load models (replace with Azure-hosted model endpoints or locally saved .h5 models)
face_model = load_model('face_emotion_model.h5')  # CNN for facial emotion
speech_model = load_model('speech_emotion_model.h5')  # LSTM for audio emotion

# Azure Speech Configuration (replace with your Azure keys)
speech_key = "<YOUR_SPEECH_KEY>"
service_region = "<YOUR_SERVICE_REGION>"
speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)

# MediaPipe for posture analysis
mp_pose = mp.solutions.pose
pose = mp_pose.Pose()
mp_drawing = mp.solutions.drawing_utils

# Facial Emotion Recognition
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

def detect_face_emotion(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    for (x, y, w, h) in faces:
        face_img = frame[y:y+h, x:x+w]
        resized = cv2.resize(face_img, (48, 48))
        normalized = resized / 255.0
        reshaped = np.reshape(normalized, (1, 48, 48, 3))
        prediction = face_model.predict(reshaped)
        emotion = np.argmax(prediction)
        return emotion
    return None

def detect_body_posture(frame):
    results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
        return "Pose Detected"
    return "Pose Not Detected"

def record_audio(duration=5, fs=22050):
    print("Recording...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    return audio.flatten()

def extract_speech_features(audio):
    mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=40)
    padded = np.zeros((40, 216))
    padded[:, :mfccs.shape[1]] = mfccs
    return padded.reshape(1, 40, 216, 1)

def detect_speech_emotion(audio):
    features = extract_speech_features(audio)
    prediction = speech_model.predict(features)
    return np.argmax(prediction)

def provide_coping_strategy(emotion_code):
    coping_strategies = {
        0: "Take deep breaths and focus on your breathing.",
        1: "Try a short meditation session.",
        2: "Talk to someone you trust or your counselor.",
        3: "Go for a short walk to clear your mind."
    }
    return coping_strategies.get(emotion_code, "Stay calm. Help is available.")

def main():
    cap = cv2.VideoCapture(0)
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        emotion = detect_face_emotion(frame)
        posture = detect_body_posture(frame)
        
        if emotion is not None:
            print("Facial Emotion Code:", emotion)
            print("Strategy:", provide_coping_strategy(emotion))

        cv2.imshow("Mental Health Companion", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    print("Capturing Audio for Speech Emotion...")
    audio = record_audio()
    speech_emotion = detect_speech_emotion(audio)
    print("Speech Emotion Code:", speech_emotion)
    print("Strategy:", provide_coping_strategy(speech_emotion))

if __name__ == "__main__":
    main()











Install required libraries:
pip install opencv-python mediapipe tensorflow keras librosa sounddevice azure-cognitiveservices-speech

Add your Azure Speech key and region.

Place your trained models (face_emotion_model.h5, speech_emotion_model.h5) in the same directory.